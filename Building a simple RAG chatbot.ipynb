{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# Building a simple RAG chatbot with LangChain, Hugging Face, FAISS, Amazon SageMaker and Amazon Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker langchain amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91613d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c025e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "\t'SM_NUM_GPUS': '1'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "\tenv=hub,\n",
    "\trole=role \n",
    ")\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29135c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7cb9",
   "metadata": {},
   "source": [
    "## Zero-shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eec468",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "As a helpful energy specialist, please answer the question, focusing on numerical data.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(system_prompt + \"{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e58740",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "\n",
    "query = f\"question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba91871",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm_chain.run({query})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG example with PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64b5bc",
   "metadata": {},
   "source": [
    "### Upload local PDF files to S3\n",
    "\n",
    "Sources:\n",
    "* https://www.iea.org/reports/world-energy-investment-2023\n",
    "* https://www.iea.org/reports/coal-2022\n",
    "* https://www.iea.org/reports/world-energy-outlook-2023\n",
    "\n",
    "Feel free to use your own files, the code below should work without any change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ba6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and prefix for PDF storage\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"langchain-rag-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17460881",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $bucket $prefix\n",
    "aws s3 cp --recursive pdfs s3://$1/$2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of S3 URIs\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "objs = objs['Contents']\n",
    "uris = [f's3://{bucket}/{obj[\"Key\"]}' for obj in objs]\n",
    "uris    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "textract_client = boto3.client('textract')\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for uri in uris:\n",
    "    loader = AmazonTextractPDFLoader(uri, client=textract_client)\n",
    "    document = loader.load()\n",
    "    chunks = splitter.split_documents(document)\n",
    "    all_chunks += chunks\n",
    "    print(f\"Loaded {uri}, {len(document)} pages, {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them in FAISS\n",
    "https://github.com/facebookresearch/faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embed chunks\n",
    "embeddings_db = FAISS.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save database\n",
    "embeddings_db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb455a",
   "metadata": {},
   "source": [
    "### Shortcut : load existing embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_db = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1db22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = \"\"\"\n",
    "As a helpful energy specialist, please answer the question below, focusing on numerical data and using only the context below.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Ask our question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223366",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the latest trend for solar investments in China?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a47bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does STEPS mean?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f083366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
